#!/usr/bin/env python3
import requests
import os
import chardet
from pathlib import Path
from urllib.parse import urlparse, urljoin

sources = []
oldSources = []
filePaths = []
printFileNames = True

def sanitize_filename(url):
    """æ›´å®‰å…¨çš„æ–‡ä»¶åå¤„ç†"""
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '')
    return domain.replace('.', '_')

def detect_encoding(content):
    """è‡ªåŠ¨æ£€æµ‹å†…å®¹ç¼–ç """
    result = chardet.detect(content)
    return result['encoding'] or 'utf-8'

def getWebsite(url, path):
    """å¢å¼ºç‰ˆçš„ç½‘é¡µä¸‹è½½å‡½æ•°"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        encoding = detect_encoding(response.content)
        try:
            decoded_content = response.content.decode(encoding)
        except UnicodeDecodeError:
            decoded_content = response.content.decode('utf-8', errors='replace')
        
        with open(path, 'w', encoding='utf-8', errors='replace') as f:
            f.write(decoded_content)
        print(f"âœ… æˆåŠŸä¿å­˜: {path}")
        return True
    except Exception as e:
        print(f"âš ï¸ é”™è¯¯: {str(e)}")
        return False

def getContent(pathToFile):
    """æ”¹è¿›çš„æ–‡ä»¶è¯»å–"""
    try:
        with open(pathToFile, 'rb') as f:
            raw_content = f.read()
            encoding = detect_encoding(raw_content)
        
        with open(pathToFile, 'r', encoding=encoding, errors='replace') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"âš ï¸ æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        return []

def add(found, lang):
    """èµ„æºé“¾æ¥æå–"""
    found = found[:found.find(lang) + len(lang)]
    for sep in [' ', '"', "'", '>', ')', ']']:
        if sep in found:
            found = found[found.rfind(sep) + 1:]
    if found and found not in sources and found not in oldSources:
        if printFileNames:
            print(f"ğŸ”— å‘ç°èµ„æº: {found}")
        sources.append(found)

def findSources(content):
    """æŸ¥æ‰¾é¡µé¢ä¸­çš„èµ„æºé“¾æ¥"""
    if not content:
        return
        
    for line in content:
        line = line.replace("'", '"')
        if ('href' in line or 'src' in line) and not line.startswith(('http:', 'https:')):
            for ext in ['.js', '.php', '.css', '.jpg', '.jpeg', '.gif', '.png', '.webp', '.htm', '.html', '.svg']:
                if ext in line.lower():
                    add(line, ext)

def download_binary(url, path):
    """ä¸‹è½½äºŒè¿›åˆ¶æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        r = requests.get(url, stream=True, timeout=15)
        r.raise_for_status()
        with open(path, 'wb') as f:
            for chunk in r.iter_content(1024):
                f.write(chunk)
        print(f"âœ… ä¸‹è½½æˆåŠŸ: {url}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {str(e)}")

def getSources():
    """ä¸‹è½½æ‰€æœ‰æ‰¾åˆ°çš„èµ„æº"""
    for source in sources[:]:
        # å¤„ç†ç»å¯¹URLå’Œç›¸å¯¹URL
        if source.startswith(('http://', 'https://')):
            absolute_url = source
        else:
            if not source.startswith('/'):
                source = '/' + source
            absolute_url = urljoin(website, source)
        
        # ç”Ÿæˆæœ¬åœ°ä¿å­˜è·¯å¾„
        if source.startswith(('http://', 'https://')):
            # å¯¹äºç»å¯¹URLï¼Œä½¿ç”¨åŸŸååçš„è·¯å¾„éƒ¨åˆ†
            parsed = urlparse(source)
            local_path = os.path.join(filename, parsed.path.lstrip('/'))
        else:
            local_path = os.path.join(filename, source.lstrip('/'))
        
        # ç¡®ä¿è·¯å¾„ä¸ä»¥/ç»“å°¾
        local_path = local_path.rstrip('/')
        
        # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if os.path.exists(local_path):
            continue
            
        try:
            if any(local_path.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.gif', '.png', '.webp', '.svg']):
                download_binary(absolute_url, local_path)
            else:
                if getWebsite(absolute_url, local_path):
                    if local_path.endswith(('.htm', '.html')):
                        filePaths.append(local_path)
        except Exception as e:
            print(f"âš ï¸ èµ„æºå¤„ç†å¤±è´¥ {absolute_url}: {str(e)}")

if __name__ == "__main__":
    website = input('ğŸŒ è¯·è¾“å…¥ç½‘ç«™URL: ').strip()
    
    if not website.startswith(('http://', 'https://')):
        website = 'https://' + website
    if website.endswith('/'):
        website = website[:-1]
    
    filename = sanitize_filename(website)
    
    if os.path.exists(filename):
        import shutil
        shutil.rmtree(filename)
    os.makedirs(filename, exist_ok=True)
    
    print(f"ğŸ“ åˆ›å»ºä¸‹è½½ç›®å½•: {filename}")
    
    index_path = os.path.join(filename, 'index.html')
    if getWebsite(website, index_path):
        content = getContent(index_path)
        if content:
            findSources(content)
            getSources()
            
            oldSources = sources.copy()
            sources = []
            
            for path in filePaths[:]:
                if os.path.exists(path):
                    content = getContent(path)
                    if content:
                        findSources(content)
                else:
                    filePaths.remove(path)
                    
            getSources()
    
    print("ğŸ‰ æ“ä½œå®Œæˆï¼")